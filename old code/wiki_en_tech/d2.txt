Part I

PROJECT DESCRIPTION
1

Identi cation of the Problem and Opportunities

The proposed study was inspired from our research and implementation of AutoTutor in the last ten years.
In AutoTutor (Graesser et al., 2001, 2004; Olney et al., 2002), and in any intelligent tutoring system (ITS), the
problems of interpreting the learner's answers and determining what the learner knows are key problems that
must be solved for the tutoring to be effective. This is specially challenging when the interaction between ITS
and students are conversational. The conversational style ITS needs to "understand" verbal contributions of
the students in order to infer what the learner knows through their answers as well as use the current model of
what the learner knows to interpret their answers. ITS needs to have world knowledge related to the content
being taught in order to understand students' verbal contribution. In ITS, the world knowledge represented
by semantic spaces (de ned and elaborated in section 3.1).
AutoTutor offers an example of using semantic spaces such as LSA in highly effective learning environment. In AutoTutor, learning content is aggregated into problem scenarios or deep level questions that
require about a paragraph of information to answer correctly. Therefore the problem of assessment becomes
the problem of comparing the state of the learner's answer (in narrative form) to the ideal answer to the
problem, which has previously been generated by a subject matter expert (SME). In AutoTutor, this comparison, as well as the instruction, is carried out piecemeal, so that at any one time, only one aspect of the
ideal answer is being discussed by the learner and the tutor. In order to assess whether the learner has mastered that aspect, AutoTutor uses LSA to compare the student's answer with its corresponding ideal answer
aspect. Thus AutoTutor computes the semantic similarity between what the student has said and what they
are expected to say to assess what they know. Similarly, the student's cognitive model is composed of the
semantic similarity between what they have said and all of the ideal answer aspects. This information is used
to dynamically sequence content based on the individual learner's progress. In both cases, the key feature of
LSA is that it compares the semantic similarity between two sets of words, the ideal answer aspect and the
learner's answer. To accomplish this feat, LSA projects words into a space, such that the semantic similarity
can be measured as the distance between words.
The effectiveness of AutoTutor is entirely depend on the quality of LSA space. It is well know that the
process of creating LSA space and evaluate quality of LSA space is labor intensive and time consuming
(as can be seen in later section 3.2.1). LSA and other similar methods of semantic encoding of written
texts are getting widely accepted and used in learning community (ref ...). Furthermore, Semantic spaces
such as LSA, HAL, Topics Model, PLSA are within the same theoretical framework(Hu et al., 2005). We
see the following opportunities that are potentially revolutionize the creation, evaluation, and application of
semantic spaces:
Ef ciency of creating and evaluating semantic spaces can be dramatically improved.
Existing semantic spaces can be re-evaluated.
New and better quality semantic spaces can be created for new learning environments.
Funded by prior NSF grants: 1)BCS 9720314 (1997-2001); 2) REC 0106965 (2001-2005); 3) IIS 0325428 (2003-2007)

Project Description – 1

2

Objectives and Expected Outcomes

Over the last decade, substantial research has investigated semantic spaces in learning contexts. Semantic spaces have primarily been used to measure learning and to provide a cognitive model of the learner.
However, as widely as semantic spaces have been used, no systematic investigation that connects theory
to experimental results has ever been performed. We propose an integrative investigation that provides a
well de ned predictive theory for semantic space creation and manipulation. Our technological goals are
to provide tools to create semantic spaces speci cally for learning assessment and cognitive modeling in
learners. Our educational foci are physics and biology domains, each of which offer different challenges
that an integrative approach must overcome. The proposed research is innovative in that it abstracts away
from traditionally held concepts concerning semantic spaces. Thus, its contribution to advanced learning
technologies is not incremental but radical.
The proposed projects will be centered around the following objectives: the technology objectives are
1. Inputting or migrating existing computer codes to the high capacity computing facility.
2. Making the codes available to the learning communities as open source
3. Creating user friendly software tools for researchers to create, evaluate, and manipulate semantic
spaces
4. Producing high quality, ready to use semantic spaces for physics and biology for the purposes of
testing the compilation of software tools and as general reference resources for the advanced learning
technology community
The educational objectives are:
1. Creating an essay grader for physics and biology using enhanced semantic spaces
2. Providing these essay graders freely as an educational resource for both students and teachers
3. Teaching graduate students about the uses and capabilities of semantic spaces

3

Research Context

According to explanation-based constructivist theories of learning (Aleven & Koedinger, 2002; Lehn et al.,
1992), an effective and deep learning environment needs to be able to guide the learner to generate explanations, justi cations, and functional procedures. To provide this kind of instruction, researchers have been
trying to develop intelligent tutoring systems that adaptively respond to the learner's actions (Graesser et al.,
2001; Anderson et al., 1995). The most challenging task in such systems is to infer what the learner knows.
Over the last decade, substantial research has investigated semantic spaces to address this challenge. Semantic spaces have primarily been used to measure learning and to provide a cognitive model of the learner.
However, as widely as semantic spaces have been used, no systematic investigation that connects theory
to experimental results has ever been performed. The next sections brie y outline the history of semantic
spaces for learning environments and identify some open questions.

3.1

The History of Semantic Spaces

The basic vector space model has been used for decades in information retrieval (Salton, 1971). In the original model, words are orthogonal, so semantic similarity judgments are too brittle to be of use in learning
Project Description – 2

environments. However, the basic model was later extended by Latent Semantic Indexing (LSI) for information retrieval (Deerwester et al., 1990). The breakthrough provided by LSI is a solution to the synonymy
problem, i.e. the problem that multiple words can express the same meaning. In the basic vector space
model, distinct words with the same meaning are kept distinct, but LSI gives them equivalent or near equivalent meanings. LSI's solution to the synonymy problem made it attractive to a variety of researchers outside
of information retrieval (Graesser et al., ress; Hu et al., 2003; Landauer et al., 1998; Olney & Cai, 2005b,a;
Wolfe et al., 1998), leading to its renaming as Latent Semantic Analysis (LSA).
Much current research use is biased by early LSI work on information retrieval. For example, issues of
term weighting and dimension selection were empirically evaluated for information retrieval and these same
parameters have been carried over atheoretically for learning environments. One particularly in uential article (Dumais, 1991) states: “Choosing the number of dimensions for the reduced dimensional representation
is an interesting problem. Thus far, our choice has been determined simply by what works best.” Likewise
this paper introduces log entropy weighting for info retrieval, which later seems to have become standard for
non-information retrieval applications (Landauer et al., 1998).
Similar atheoretical positions were taken as LSI became LSA and was used for non-information retrieval
purposes. The assumed “optimal” number of dimensions was declared as 300 (Landauer & Dumais, 1997).
Despite this lack of guiding theory, LSA has been very successful. Applications include modeling sentence
comprehension (Kintsch, 1998), synonym testing (Landauer et al., 1998), word sorting (Landauer et al.,
1998), essay grading (Landauer et al., 1998), and predicting learning from text (Wolfe et al., 1998). LSA has
been particularly successful in intelligent tutoring systems such as AutoTutor.

3.2

Semantic Space Formal Overview

LSA belongs to a general class of vector space models. The vector space model is a statistical technique
that represents the similarity between collections of words as a cosine between vectors (Manning & Schütze,
1999). The process begins by collecting text into a corpus. A matrix is created from the corpus, having one
row for each unique word in the corpus and one column for each document or paragraph. The cells of the
matrix consist of a simple count of the number of times word i appeared in document j. Since many words
do not appear in any given document, the matrix is often sparse. Weightings are often applied to the cells that
take into account the frequency of word i in document j and the frequency of word i across all documents,
such that distinctive words that appear infrequently are given the most weight. Two collections of words of
arbitrary size are compared by creating two vectors. Each word is associated with a row vector in the matrix,
and the vector of a collection is simply the sum of all the row vectors of words in that collection. Vectors are
compared geometrically by the cosine of the angle between them. This geometric interpretation is likely the
reason that vector space models have become so popular: the interpretation is simple, clean, and elegant.
LSA (Deerwester et al., 1990; Landauer et al., 1998) is an extension of the vector space model that uses
singular value decomposition (SVD). SVD is a technique that creates an approximation of the original word
by document matrix. After SVD, the original matrix is equal to the product of three matrices, word by
singular value, singular value by singular value, and singular value by document. The size of each singular
value corresponds to the amount of variance captured by a particular dimension of the matrix. Because
the singular values are ordered in decreasing size, it is possible to remove the smaller dimensions and still
account for most of the variance. The approximation to the original matrix is optimal, in the least squares
sense, for any number of dimensions one would choose. In addition, the removal of smaller dimensions
introduces linear dependencies between words that are distinct only in dimensions that account for the least
variance. Consequently, two words that were distant in the original space can be near in the compressed
space, causing the inductive machine learning and knowledge acquisition effects reported in the literature
(Landauer & Dumais, 1997).

Project Description – 3

3.2.1

Complexity of Creating and Evaluating Semantic Spaces

The process of creating and evaluating a semantic space involve several critical steps. For example, to create
a LSA space, there are total of seven steps:
1. Selection of a domain. This is determined by the purpose of semantic space. For example, a semantic
space may be used as semantic engine for a tutoring systems of a speci c domain (Franceschetti et al.,
2001) or for grading essays in a give subject(Foltz et al., 1999).
2. Selection of corpora. This step involve collect and select relevant texts for the given domain. For
example, if domain is "physics", there is a choice of textbooks, research articles, or both.
3. Processing of row material. It is possible that collected material contains not only text, but also graphics (bitmap of a scanned document). Even the material are texts, there may be strings that are tags and
attributes (For example, in html les). This step also include inserting proper sentence or paragraph
markers. In some cases it is at this step where researcher may need to decide the size of document.
4. Obtain Term-document frequency matrix A. at this step, one needs to decide the value of each entry
of the matrix as a function of global weight gi (weight of the word i) and local weight lj (weight of the
document j) and the frequency of word i in document j; fij : It usually takes the form of
(A)ji = gi lj fij

(1)

Notice that gi , lj , and fij in (1) can take different forms.
5. Normalization of the Term Document matrix: Some of the documents in the corpus might be of
unequal length, so the frequency of a term in a shorter document will not be homologous to the
frequency of a term in a longer document. So normalization must be done on the term-document
matrix to bring all the columns of the matrix to an equal scale so that they may be compared. The form
of normalization which popular for LSA based applications is Tf-Idf [9].
6. Decomposition of the word-document matrix and represent words by vectors. There are different
ways one could process the matrix. Even SVD is most popular method used, one could use others to
represent words as vectors.
7. Dimensional reduction. This is the step to determine how many dimensions are enough for a vector
representation. It is reported that 300 is the best for some applications (ref.). However, there are cases
in which higher dimensions are used.
8. Processing of the vectors. This is the step to decide
(a) if the rst dimension of the word vector need to be removed, and
(b) if the dimensions need to be weighted(Hu et al., 2003).
After the all the steps, each word is represented as a n dimensional vector with n be a number chosen
at the step 7: There are also different ways users use the vectors to compute similarity.
1. Similarity computation. Cosine is the default similarity measure for LSA. It was also suggested that
other similarity measures (i.e., Mahalanobis distance measure, Jeffry Divergence etc.) can be used.
2. Use of the similarity value. In the case of comparing similarity values between documents, one could
use the value by itself, one could use the value and considering the size (number of words) of the
document.
Project Description – 4

The geometrical interpretation provided in [S. T. Dumais, "Latent Semantic Analysis," Annual Review
of Information Science and Technology (ARIST), pp. p189-230, 2004] gives an insight into the underlying
principles of LSA. Also this interpretation highlights the difference between vector space models and LSA
based approach. The geometrical interpretation is shown in 1

Figure 1: The above depicts the geometrical view of vector-space model. Terms are the axes and the documents are vectors in the n dimensional space, where n is the number of terms. The axes are pretty rigid, for
example, document 3 that contains only term 1 cannot be retrieved by term 2 as query even though there is
a semantic similarity between both of them. Figure 1.2 depicts the geometrical interpretation of the LSA.
After performing the dimensionality reduction step, the terms are no longer the axes in the space, which are
substituted by LSA dimensions (which are less than the number of terms). Now the terms are also vectors
in the LSA space and so they are not independent as previous, therefore a query can match documents even
though the documents do not contain the query terms.
Other semantic spaces such as HAL(Burgess, 1998) and NLS(Cai et al., 2004) are only different with
LSA in some of the steps (such as steps 5, for example). In general, there are many parameters need to be
determined for any semantic space before it is evaluated by human experts. Evaluation of LSA space with
given set of parameters are conducted by comparing LSA with human experts on some benchmark task. For
example, use LSA to compute similarity between texts and then use human to judge the appropriateness of
the outcome (usually using simply correlational analysis). Human experts' evaluation of semantic space is
time consuming. Simple calculation shows that there is impossible for human experts to evaluate semantic
spaces with all the possible combinations of parameters.
From the simple analysis in this section, we conclude that it is impossible for human experts to evaluate
semantic spaces when all the parameters are considered. Our goal is to propose a computational solution. In
the next section, we introduce some basic de nitions and assumptions for semantic spaces.

Project Description – 5

4

Proposed Research

The proposed projects are based on the most recent (Hu et al., 2005) work done by PI and Co-PI. This is a
mathematical framework that characterizes most semantic spaces such as LSA, HAL, and NLS. The basic
idea is to formula semantic spaces as an algebraic structure with similarity measures. The most important
concept in this theory is the concept of induced semantic structure (ISS) for a semantic space. With ISS,
comparisons can be made between semantic spaces. Such comparisons are operationally de ned within the
framework, and they are numerical. Based on these measures, we can achieve what we have proposed in this
proposal. Next we provide some basic assumptions and de nitions for the theoretical framework of semantic
spaces. We rst brie y outline some basic concepts from Hu et al. (2005) and then turn to the description of
the proposed projects.

4.1

Theoretical Foundation

Hu et al. (2005) framework of Semantic Space propose 1) a general de nition of Semantic Space. This
de nition characterizes most widely used semantic spaces such as LSA, HAL, and NLS. 2) Induced semantic
structure of a semantic space. 3) Three levels of measurements between semantic spaces.
4.1.1

De nition of Semantic Space

Hu et al. (2005) proposed a formal de nition for semantic spaces (De nition 1of Hu et al. (2005)). From Hu
et al. (2005), semantic spaces such as LSA satisfy following three assumptions:
1. Representational Assumption: Semantics of any level of the language entities can be represented
numerically or algebraically
2. Hierarchical Assumption: Semantics of different levels of the language entity may be represented
differently, but semantics of a higher level language entity is computed as a function of semantics of
its lower level language entities.
3. Computational Assumption: Semantic relations between any two same level language entities can be
numerically measured as a function of the semantics of the lower level language entities.
With the above assumptions, Hu et al. (2005)'s framework further assumes that semantics of any term is
de ned by its (numerical) relations with other terms in the same semantic space.
4.1.2

Induced semantic structure of a semantic space

Within Hu et al. (2005)'s framework, semantic spaces can be examined at a level that is beyond the level of
representations. For example, one could examine LSA spaces with different representations (300 dimensions
or 500 dimensions), or even with different corpora (6th grade text or 9th grade text from TASA, for example).
Such comparison between semantic spaces is made possible by the concept of induced semantic structure
within Hu et al. (2005)'s framework. It is assumed that there is a similarity measure between any two
elements within the same level. That means that any two items can be compared. Given any element,
(partial) order can be assigned to all other elements in the same level. Induced semantic structure is the
partial ordering relations between elements (in the same level, such as words, phrase, or sentence).

Project Description – 6

4.1.3

Three levels of measurements between semantic spaces

With the concept of induced semantic structure, further assumptions are made within Hu et al. (2005)'s
framework. These assumptions serve as the theoretical foundation for the similarity measures of semantic
spaces.
1. The meaning of a word is embedded in its relations with other words. as an illustrative example (see
Fig 1), the word "life" has different near neighbors for different LSA spaces.
2. If a given word is shared in different semantic spaces, the relation between the semantics of the word
in different semantic spaces is a function of the corresponding induced semantic structures.
3. The relations between any two semantic spaces are a function of the relations of the semantic structures
of all the shared words.
With the above assumptions, we are able to measure similarity between semantic spaces at three different
levels: Combinatorial Similarity, Permutational Similarity, and Quantitative Similarity.
1. Combinatorial Similarity: Based on the concept of induced semantic structure, for each element, there
will be a corresponding ordering relation imposed upon other element in the same level. Combinatorial
Similarity between any two elements is computed as a function of the number of overlap elements for
the top (say, T ) items. It obviously depends on the number of elements we consider. It can simply be
considered as the overlap of the top T elements in the near neighbors.
2. Permutational Similarity: While Combinatorial Similarity only considers the overlap of the top T
neighbors, Permutational Similarity further considers the order of the top neighbors.
3. Quantitative Similarity: Combinatorial Similarity and Permutational similarity are based on algebraic
properties of the induced semantic structure as a partial order. Quantitative Similarity is based on
quantitative property of the nearest neighbor.
The computational details and mathematical properties of the three measurements are too much information to elaborate here in the proposal. However, we would want to point out the following important
fact: All the three levels of similarity measures are de ned for single element. It can be extended to a set of
elements. For example, if we consider a set of physics glossary items and explore semantic of these items
in two semantic spaces, assuming these glossary items are contained in both semantic spaces. For each of
the items, there is a value at of the each of the three levels. For the set of the glossary items, there will be
three sets of values one corresponding to each level of the measures. In this case, statistical properties of the
three sets of measures (such as mean and standard deviation) are used to measure semantic similarities for
the entire set of the glossary items. Usually the three levels of measures can be obtained for any set of items
W; where W can be any subset of the intersection of the same level elements from the two semantic spaces.

4.2

Open Questions in Semantic Spaces

The preceding sections have highlighted some of the more obvious open questions regarding semantic spaces
and their corresponding usage in learning environments. However, there are also a number of more subtle
open questions that the proposed research will address. The sheer number of open questions and the lack
of integrative theory illustrates the need for the proposed research if the science and technology of semantic
spaces for learning environments are to progress.
Weighting Previous research in information retrieval has only shown the effects of weighting experimentally for a particular corpus.
Project Description – 7

Dimensions Previous research in information retrieval has only shown the effects of retained dimensions
experimentally for a particular corpus. Moreover, the properties of the individual dimensions themselves have not been adequately analyzed. It has been shown that the rst dimension of LSA spaces is
always large and positive, supporting the argument that dimensions should be examined individually
for optimal results (Hu et al., 2003).
Resolution All previous research in LSA has used the term-document matrices described in Section 3.2.
However, there has been no exploration of what should count as a term or document. Therefore it is an
open question whether a term should be a word or a series of words (n-gram). Likewise it is an open
question whether a document should be a sentence, a paragraph, or some larger unit.
Corpora There is only sparse research on the corpus properties needed to make a good semantic space. For
example, there could be an ideal ratio of content words to documents. Some early results suggest that
it is better to use a small high quality corpus than a larger, less focused one, but these results are not
conclusive (Franceschetti et al., 2001)(Olde et al., 2002)
Preprocessing Previous work in information retrieval has investigated the impact of stemming (Porter,
1980), but little work has been done on the effect of analogous preprocessing on semantic spaces.
For example, the effect of separating morphemes like “didn't” into “did n't” in general for semantic
spaces is unknown.
Similarity Finally, the similarity measure itself, cosine, has not been systematically evaluated for semantic
spaces. There are a number of other possible similarity measures that may be more appropriate given
the nature of the task or the structure of the corpus.
Although a few of these questions have been addressed in previous research, there are two common
defects in the knowledge that has been gained. First, for historical reasons, the domain under which these
questions have been investigated is frequently information retrieval. Information retrieval is a markedly
different problem than the semantic similarity judgments required by advanced learning environments. Thus
it is not clear whether an optimal practice for information retrieval will be optimal for learning environments.
Secondly, these questions have often been addressed in an ad hoc manner, so as to produce a set of optimal
parameters for a speci c problem rather than any kind of theoretical framework. As such, their utility is rather
limited when considering semantic spaces in general, and their results can only be taken as suggestive. On
the other hand, the proposed research will create an integrative theory that links the mathematical properties
of semantic spaces to experimental results. This theory will bridge results created in disparate domains, and
lay a foundation for future breakthroughs in semantic spaces.

4.3

Applications of the theoretical framework

In the previous section, we have introduced one major concept of induced semantic structure (Section 4.1.2)
and three levels of similarity measures: combinatorial, permutation, and quantitative similarities between
items and between spaces (statistical properties of sets). The proposed research is based on the formal
framework of induced semantic structure and these measures. To illustrate this framework, we give a simple
example of measuring semantic similarity at the item level and outline detailed procedures for measuring
semantic similarity at the space level.
4.3.1

Example: Meaning of "life" in different LSA spaces

Assume that we have two LSA spaces L1 and L2 with a common set of words W . Two matrices can be
obtained by considering near neighbors (Section 4.1.2) for all words in W : S1 and S2 , such that each
Project Description – 8

corresponds to the similarity measure of between all words in W for the two LSA spaces L1 and L2 . Thus
S1 and S2 are square, symmetric, and have dimension jW j. Notice that these two similarity matrices contain
all necessary information needed for combinatorial, permutation, and quantitative similarities measures.
In this example, we compute combinatorial, permutation, and quantitative similarities for the word “life”.
Table 1 lists near neighbors for several LSA spaces. We computed combinatorial, permutation, and quantitative similarities with the value T = 50 (see Tables 2, 3, and 4); We observed that the meaning of “life”
is most similar between 6th grade and 9th grade and between 9th grade and 12th grade in the Touchstone
Applied Science Associates (TASA) corpus.
It is very important to notice that
1. Values computed from the equations for combinatorial, permutation, and quantitative similarity are
functions of the value T: By varying T a more optimal structure may be induced for a particular
semantic space.
2. The use of combinatorial, permutation, and quantitative similarity are at the smallest resolution of
the LSA spaces, namely, the items. They can also be applied to the levels of phrases, sentences, or
documents. For example, one may evaluate the meaning of one document across different spaces. We
will outline this as a project in later section.
4.3.2

Measuring semantic differences for collection of items between semantic spaces

In section 4.1, the descriptions for combinatorial, permutation, and quantitative similarity only provide similarity measures between items. In the example, we have used the descriptions for combinatorial, permutation,
and quantitative similarity to measure “meaning of life” between different semantic spaces. If a collection
of items are considered, numerical values can be obtained for each of the terms considered. For example, if
W is a collection of furniture items, then the combinatorial measure would be a collection of combinatorial
similarities for those items between two target semantic spaces. Statistical properties (such as mean and
standard deviation) can be obtained to measure the differences. Although W is only a collection of items, it
is very important to know that
1. If W is the overlap of all items in the target semantic spaces, statistical properties of combinatorial,
permutation, and quantitative similarity will re ect semantic differences between the target semantic
spaces with the given parameter T:
2. W can be a collection of phrases, sentences, and documents. One of the proposed projects will be to
examine difference between two completely different semantic spaces with same corpus at the level of
documents.
4.3.3

Selecting “best” semantic space for advanced learning environment

We have outlined methods to measure differences between semantic spaces: at the level of individual items
(such as the item “life”); at the level of collection of items; or at the level of an entire semantic space. The
ultimate goal is to select the “best” semantic space for the application. Based on theoretical framework in
Section 4.1 and methods outlined in the previous two sections, the selection of “best” semantic space for a
speci c application is actually straightforward. The key is to have an induced semantic structure that best
re ects human knowledge of the given domain. If such a "gold standard" induced semantic structure is
obtained, then all we need to do is to select best set of parameters to produce a semantic space with the
minimal difference with the "gold standard".
There are several important facts that in uence the selection processes and outcomes:

Project Description – 9

1. Quality of "gold standard" induced semantic structure for a given domain. From Section 4.1.2 that the
induced semantic structure is simply a N by N matrix ( N is the number of items at the given layer
of a semantic space).
2. Quantitity T (number of top neighbors to use in computation) and W; the set of elements used.

Table 1: Top 20 nearest neighbours of "life" for different LSA spaces (only showing 6th–12th). The rank is
taking from http://lsa.colorado.com.
6th
9th
12th
life
life
life
reincarnation contemplated
death
premiums
reincarnation
lifetime
policyholder
sai
hamlin
premium
pipal
pipal
sai
nirvana
nirvana
cycles
lifetime
zarathustra
holdover
death
ahuramazda
condemning
hinduism
ahriman
chekhov
afterlife
policyholder
captial
excerpted
romantics
pipal
ribman
essayists
nirvana
reaf rm
sai
hinduism
militarily
beaumarchais
span
kindless
1658
priori
condemning
pseudonym
maturity
premiums
poquelin
immoral
premium
ribman
humane
policyholder
kindless

Table 2: Combinatorial similarity of the word "life" between several LSA spaces.
3rd
6th
9th
12th
College
3ed
1
6th
0:0526 1
9th
0:0204 0:2987 1
12th
0:0000 0:0989 0:2500 1
College 0:0000 0:0989 0:1111 0:2048 1

4.4

Projects

Given the theoretical framework provided in Section 4.1 and basic methods in Section 4.3, we will be able to
solve the challenge posed in Section 3.2.1. In this proposal, we propose three types of projects: 1) Computer
implementation, 2) Evaluating semantic spaces, and 3) Apply to advanced learning environments.

Project Description – 10

Table 3: Permutational Similarity of the word "life" between several LSA spaces.
3rd
6th
9th
12th
College
3ed
1
6th
0:0491 1
9th
0:0136 0:2078 1
12th
0:0000 0:0478 0:2130 1
College 0:0000 0:0462 0:0626 0:1450 1
Table 4: Quantitative Similarity of the word "life" between several LSA spaces.
3rd
6th
9th
12th
College
3ed
1
6th
0:0524 1
9th
0:0202 0:2975 1
12th
0:0000 0:0979 0:2495 1
College 0:0000 0:0975 0:1101 0:2043 1
4.4.1

Computer implementation

The most essential component of this proposal is the use of high capacity computers to carry out huge amount
of computation. The University of Memphis High Capacity Computing Center has recently installed an IBM
super-computer. We propose to utilize this cutting edge facility to conduct all needed computation. In order
to use the facility for the computation, we divide the "computer implementation" into a serious of small
projects:
Improvement of LSA Creating Latent Semantic Analysis (LSA) utility for the super-computers. LSA
is the most studied type of semantic space that has been used in ALE. Especially, it has been used in all
ALE developed at the Institute of intelligent Systems (AutoTutor, iStart, and Cohmetrix). This task will be a
combination of migrating existing LSA algorithms currently in use and new components that are parameter
controlled. With the new LSA generating software, LSA space can be generated in a matter of minutes based
on any speci cation of the seven steps (in Section 3.2.1).
Implement other Semantic Spaces
et al., 1996), NLS(Cai et al., 2004).

Create software to generate semantic spaces such as HAL(Burgess

Generate nearest neighbors for semantic spaces Creating programs that can generate neighbors of any
given item x (Section 4.1.2). This is a program that will require heavy computing. It is actually creating a
N by N matrix whose entries are similarity computed within a given layer of a semantic space, where N is
the number of items in the give semantic space. Such a matrix will be used to generate the induced semantic
structure (Section 4.1.2) and the three levels of similarity measures.
Computing similarity measures Creating programs that can compute the three levels of similarity measures for any two elements in the same layer of either the same semantic space or different semantic spaces.
The above four projects are computational in nature and require huge number of computations. We plan
to nish projects 4.4.1, 4.4.1, and 4.4.1 in the rst year and (4.4.1) in the rst half of the second year.

Project Description – 11

4.4.2

Evaluating semantic spaces

We expected to nish three of the four computer implementation projects 4.4.1, 4.4.1, and 4.4.1. With the
completion of project 4.4.1, we will be able to start evaluating semantic spaces. The following projects are
expected to nish in the second year. the projects are separated into two parts: Systematically evaluate LSA
spaces and Selecting "best" semantic space for advanced learning environment.
Systematically evaluate LSA spaces To systematically evaluate LSA spaces, we propose to generate LSA
spaces with different parameters. Our plan is to vary parameters according to the open questions identi ed in
Section 4.2. This will involve creating a large number of LSA spaces and computing pairwise comparisons
between them. In each of the paired comparison, there will be three levels of similarity measures(Section
4.1.3). Each of the similarity measures will be a function of the parameter T (the number of items in the
nearest neighbor). There will be different T used in obtaining similarities between items. The comparison
between spaces are statistical properties of the set of similarity values which is also determined by W (the set
of items that are common to both spaces to compare). We will have three possible cases: 1) W is collection
of all the words, 2) W is the set of words with global weighting gi (Equation (1) ) exceeds certain threshold,
and 3) W is a set of domain speci c (such as physics) glossary items. We expect to spend the entire second
year to nish analysis. It is possible that we will be able to consider other variations.
Selecting "best" semantic space for advanced learning environment The goal of this project is to select
parameters so the semantic space will have the smallest difference from a "gold standard" for a given domain.
Our rst step is to generate a "gold standard" from well established database currently used. It involves the
creation of a portiony of the N by N matrix (induced semantic structure, (Section 4.1.2)). and compute
similarity between the semantic space with given set of parameter and the "gold standard".
Selection of semantic spaces based on word association norms Word association norms specify the
strength of association (a numerical value) between word pairs. The information contained in the word
association norms may not be enough for the whole N by N matrix that is speci ed in Section 4.1.2, the
comparison between a semantic space and the "gold standard" can be achieved by selecting W and T so all
information from the word association norms can be used.
There are several very well collected databases on association norms. The Edinburgh Associative Thesaurus contains 8400 stimulus words and contains 55732 words in total. Another database if the University
of South Florida Word Association, Rhyme and Word Fragment Norms, collected by Nelson, D.L., McEvoy,
C.L. and Schreiber T.A. It contains 5,019 stimulus words and about 0.75 Million responses, from more than
6,000 participants.
Selection of semantic spaces based on Wordnet WordNet is a lexical reference system developed
by the Cognitive Science Laboratory at Princeton University. WordNet organized English nouns, verbs,
adjectives and adverbs into synonym sets, each representing one underlying lexical concept. The synonym
sets are linked by different relations, such as antonym, hypernym, entailment, etc.
The WordNet relations can be used to derive other relations. For example, the hypernym is a relation
specifying that a word is a kind of another word(e.g. a “chair” is a kind of “seat”). We can derive a hypernymlinked relation to two words that share a same hypernym. Thus, “chair” and “sofa” have a hypernym-linked
relation because they share a hypernym “seat”. So do “chair” and “table” because they share a hypernym
“furniture”. With such relations, we can test the semantic space to see how a space or a similarity measure
y

It is not realisitic to obtain the entire N by N matrix for all the elements in a given layer, if the entries are obtained from human
experts.

Project Description – 12

predicts a given relation. The way of measuring the strength of a semantic space in predicting a relation
needs further investigation.
Selection of semantic space based on Microsoft Paraphrase database The Microsoft Research Paraphrase Corpus ()Dolan et al. (2004) will be used to select semantic spaces at the sentence level. This corpus
contains 5801 sentence pairs that have been judged to be either semantically equivalent or semantically
non-equivalent. Three human raters produced judgments, with an average inter-rater reliability of 83%. Approximately 67% of the sentence pairs were found to be equivalent. Since the non-equivalent pairs overlap
in both information content and wording, discrimination is a dif cult task. All sentence pairs were extracted
from online news sources over a period of several months.
Selection of semantic spaces based on domain experts Domain experts will contribute detailed association among a carefully selected set of words. Speci cally, association strength between a set of glossary
terms in physics will be obtained from physicist professors and graduate students. Based on the association
strength among the glossary terms, the induced semantic structure will be created. We will use the induced
semantic structure as "gold standard" to select best set of parameters for a semantic space. For example,
by applying appropriate local weighting ( lj , as in Equation (1)) that speci cally emphasis the domain of
physics, we might obtain an LSA space generated form TASA to use in the physics instance of AutoTutor.
4.4.3

Apply to advanced learning environments

The projects we outlined in the rst two sections will nally be used to serve our ultimate purposes of this
ALT grant:
1. Produce tools/utilities to evaluate semantic spaces and
2. Implement "best" semantic space in a real advanced learning environment.
3. Our nal set of projects proposed here two essay graders, one in the domain of physics and one
in the domain of biology. These two different domains have been chosen based on assumptions of
differences between their semantic structures. For example, it is assumed, based on previous work
on AutoTutor for physics, that the ordering of words is an important issue for essay grading, more so
than for biology. We believe this relates directly to causal explanations in physics and the associated
ordering of words. Biology poses other interesting challenges and is distinct from physics in the size
of its vocabulary. For example in biology, many concepts are represented by a distinct word that might
be composed of many morphemes such as "heterozygous" which is composed of "hetero", "zy", and
"gous". Automatic essay grading in these two domains is a useful benchmark tool for semantic space
performance. Additionally, it provides a valuable resource for students who want feedback on their
essays and for teachers who might want a second opinion on a student's essay.

5

Institutional Commitment, Dissemination, and Impacts

Resources existing across four academic units (Computer Engineering, Psychology, Institute for Intelligent
Systems, and Super Computing facility) will facilitate the research and increase potential for dissemination of results. The design of robust and computationally ef cient framework in developing the theory and
advanced learning technologies using semantic spaces concepts to teach computers to understand humans
remain elusive and dif cult to achieve. However, some aspects of this problem can be addressed by accommodating the semantic space representation of semantic similarity as distances between words. In addition,

Project Description – 13

the interplay between learning, cognitive modeling can be used to develop and asses a variety of learning
environment tasks. In a recent work, the PI (Hu) and collaborators [(Hu et al., 2005)] has done empirical
analyses on mathematical framework that characterizes most semantic spaces such as LSA, HAL, and NLS.
It was noted that semantic similarity can be modeled using an algebraic structure by using the concept of
induced semantic structure (ISS) for a semantic space. With ISS, comparisons can be made between semantic spaces. Such comparisons are operationally de ned within the framework, and they are numerical.
The current proposal advances the idea of semantic space similarity by proposing a uni ed framework of semantic space using: (a) a general de nition of semantic space; (b) Induced semantic structure of a semantic
space; (c) Three levels of measurements between semantic spaces as discussed in the section 3. Based on
these measures, the proposing team is con dent to achieve the stated goals and objectives. The outcome of
the proposed research may have profound impacts on deeper understanding of semantic space similarity in
comprehending human language as well as advanced learning technologies.

5.1

Research and Learning

One of the main objectives and expected outcome of this project is to develop an ideal environment for interdisciplinary research and learning in computer engineering, cognitive science, psychology and linguistics.
Understanding will be advanced and learning will be achieved through the integration of research activities
in courses, web-based educational demos and through undergraduate and graduate research. The outcome
of the research will be taught in existing courses (i.e., EECE7905/8905: Multimedia Information Processing and Retrieval) at the University of Memphis. In addition to advancing research on semantic space, and
advance learning technologies, the research will broaden the understanding of students in cognitive science,
natural language processing, computational linguistics, arti cial intelligence, data mining, distance learning,
and information technologies. It thereby advances research in engineering, psychology, computer science,
and a range of other interdisciplinary areas.

5.2

Integration with existing curricula

Our proposed interdisciplinary course EECE7905/8905 is important because it emphasizes critical thinking
skills, problem-solving techniques, and a team approach to building applications while lling the need for
students acquiring fundamental semantic space analysis techniques that can be integrated into subsequent
graduate engineering and science courses, capstone projects, and honors theses. The proposed course will
eliminate the void in applications of advanced learning technology that exist throughout the engineering and
science curricula at both U of M and within the Mid-South region while complementing the programming
courses that our students already take by reinforcing programming techniques and having students contribute
to applications that will provide insight into phenomena captured in large data sets.

5.3

Civic engagement and educational outreach

Few high school teachers and even fewer high school students have any exposure to advanced learning technologies used by engineers and scientists. Semantic understanding and comprehension of natural languages
offers a new dimension in communicating the excitement of science and vividly introducing K-12 educators to both the richness and perplexing complexity of natural and technological processes. By promoting
interactions between undergraduate students and high school teachers to build or enhance teachers will be
in uenced in understanding the physical world. Enlisting a signi cantly larger number of engineering and
science students and faculty in collaborations with high school teachers is an additional outcome.

Project Description – 14

5.4

Impact to local/regional community

More here

6

Management Plan

The proposing team will be responsible for identifying and advancing research and the day to day management of students and collaborators. The PI and the project director, Dr. Hu has conducted extensive research
in cognitive science, the psychology of language and intelligent tutoring systems. Dr. Hu will be responsible
overall quality of the proposed study. Dr. Yeasin has been expert in creating learning environments. He will
perform the development of computationally ef cient algorithms and integration of algorithms to develop
a successful learning technologies and their evaluation. Mr. Cai is a trained mathematician and computer
architect for the Institute of Intelligent Systems at the University of Memphis. He has produced numerous
tools and utilities that analyzes written texts. His role will be mathematical analysis of semantic spaces and
direct students to producing computational linguistics modules. Dr. Xu is an expert in evaluation in educational settings. Her role will be oversee experimental design, data collection, and analysis of the software
utilities produced from the projects.
All members of the team have studied extensively various aspects of semantic spaces. Hu, Yeasin, and
Cai have published on semantic spaces and learning environments. The team has the technical expertise necessary to build and test the proposed software testbed. Hu and Cai have all used, developed, and/or tested semantic space software for prior research projects. There have been numerous research collaborations among
the PIs on current and previous projects. Hu, and Cai have worked together for 4 years studying semantic
spaces, human and automated tutoring, question asking and answering, and animated agents. Additional information about the team's expertise and synergistic activities are provided in supplemental documentation
(i.e., NSF BioSketches). Dr. Xu offers expertise from the angle of education. Her expertise in research
design and analysis from point of view of education made this a project ts best to the mission of ALT.

6.1

Meetings

There will be two weekly research meetings: one meeting will focus on the research challenges in the
development of semantic space theories and evaluation issues and the second meeting will discuss interdisciplinary issues in integrating the semantic space theories to develop software testbed for advance learning in
the context of the proposed research. These meetings augment the normal schedule of courses and seminars.
Since all PIs are at the same institution, they will also have less formal meetings in a shared lab space.

6.2

Integrative Research Plan

Our plan is to build on the PIs expertise in learning technologies and semantic spaces to integrate theoretical
and experimental results. This will be an iterative process within and between phases of research. In particular we plan to use the University of Memphis supercomputer to explore uncharted territory in semantic
spaces. This exploration will help us locate the important phenomena to be explained by our theory. Likewise our theory will both explain the observed phenomena and lead us to ask new questions, which we will
experimentally verify. We believe that this tight linkage between theory and experiment is essential to the
success of the project.

Need to create a gant chart
Major Project Goals

Major Milestone Description
Project Description – 15

Year 1

Year 2

Year 3

First Year
Computer Implementation
1. Creating Latent Semantic Analysis (LSA) software for the super-computers
2. Creating software for generating nearest neighbors for semantic spaces
3. Creating programs that can compute the three levels of similarity measures
Creation of gold standard induced semantic spaces by human experts for biology and physics
Second Year
Implement other Semantic Spaces
Evaluate Semantic Spaces by generating spaces with different parameters
Selecting best semantic spaces
1. Based on word association norms
2. Based on WordNet
3. Based on the MSR paraphrase corpus
4. For advanced learning environment using a gold standard human generated induced semantic
space
Third Year
Apply to advanced learning environments
1. Collect essay grading data in domains of physics and biology
2. Create optimum spaces for these domains using the techniques and tools from years 1 and 2
3. Create essay grading software based on these spaces
4. Evaluate the performance of these spaces for this task
Open up software developed on the project to the public domain

Project Description – 16

7
7.1

Summary: Signi cance of proposed work
Intellectual Merit

In order to advance the science of semantic spaces for advanced learning technologies, an integrative theory
is essential. The investigators have extensive experience with the use and evaluation of semantic spaces in
advanced learning environments. Moreover, they have the necessary quali cations to create an integrative
mathematical theory of semantic spaces. The investigators have access to a newly built supercomputer
for the proposed research. The proposed research is innovative in that it abstracts away from traditionally
held concepts concerning semantic spaces. Thus, its contribution to advanced learning technologies is not
incremental but radical.

7.2

Broader Impacts

Because semantic spaces are a core component of many advanced learning environments (ALE), discoveries
made during the proposed investigation should improve the learning outcomes of these ALEs. Since many
ALEs are on the Internet, these effects will transcend geographic boundaries and be available across the
population, including underrepresented groups. The proposed investigation's integrative theory and tools
will provide a valuable resource for further research in education. These research products will be made
freely available on the Internet and disseminated to the broadest possible audience, as well as being the
subject of journal articles and proceedings. Overall, society will bene t from the resulting improvements in
ALE, which will have positive effect on education and productivity.

7.3

Education and Training

Graduate and undergraduate students at the University of Memphis will receive major parts of their training via funding for this project, carrying out dissertations, MS and senior theses based on the proposed
studies and technology development. The learning and training experiences of graduate and undergraduate
students will be enhanced in that students at both levels will be actively involved in all phases of the research
process. Students will be actively involved in the experimental design of studies, data collection and analysis,
computer programming, and reporting of the work at conferences and in refereed publications. Additional
students will be exposed to the research process in special interdisciplinary research courses that are offered
at the University of Memphis. These course lectures will introduce and involve students in various phases
of the research and technology development process. Graduate and undergraduate students will also receive
support for travel to conferences and the teaming institutions, which will educate them in the mechanics of
professional collaboration.

7.4

Dissemination Plans

Insights and results that emerge from the research will be presented at conferences (e.g., AIED [Arti cial Intelligence in Education], CHI [Computer-Human Interaction], Psychonomics, Society for Text & Discourse,
Cognitive Science, Association for Survey Computing, ITS [Intelligent Tutoring Systems]) and published
in journals from the relevant disciplines (e.g., Journal of Experimental Psychology: Applied, Cognition,
Social Behavior and Personality, Human-Computer Interaction, Journal of Educational Psychology, Cognitive Science, Computers and Human Behavior, Discourse Processes, Speech Communication, Language
and Speech, and International Journal of Arti cial Intelligence in Education). In addition, we will make the
software tools available to researchers who want to integrate it into their own systems or simply use it as a
standalone tool to analyze and create semantic spaces.

Project Description – 17

Part II

REFERENCES CITED
References cited
Aleven, V. & Koedinger, K. R. (2002). An effective metacognitive strategy: Learning by doing and explaining with a computer-based cognitive tutor. Cognitive Science, 26, 147–179.
Anderson, J. R., Corbett, A. T., Koedinger, K. R., & Pelletier, R. (1995). Cognitive tutors: Lessons learned.
Journal of the Learning Sciences, 4, 167–207.
Burgess, C. (1998). From simple associations to the building blocks of language modeling meaning in
memory with the hal model. Behavior Research Methods, Instruments, and Computers, 30, 188–198.
Burgess, C., Livesay, K., & Lund, K. (1996). Modeling parsing constraints in high-dimensional semantic
space: On the use of proper names. In Proceedings of the Cognitive Science Society. Hillsdale, N.J.:
Erlbaum.
Cai, Z., McNamara, D. S., Louwerse, M., Hu, X., Rowe, M., & Graesser, A. C. (2004). Nls: Non-latent
similarity algorithm. In Forbus, K., Gentner, D., & Regier, T. (Eds.), Proceedings of the 26th Annual
Meeting of the Cognitive Science Society, (pp. 180–185). Mahwah, NJ: Erlbaum.
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A. (1990). Indexing by
latent semantic analysis. Journal of the American Society of Information Science, 41(6), 391–407.
Dolan, W., Quirk, C., & Brockett., C. (2004). Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of COLING 2004, Philadelphia. Association for
Computational Linguistics.
Dumais, S. (1991). Improving the retrieval of information from external sources. Behavior Research Methods, Instruments and Computers, 23(2), 229–236.
Foltz, P. W., Laham, D., & Landauer, T. K. (1999). Automated essay scoring: Applications to educational
technology. In proceedings of EdMedia '99.
Franceschetti, D., Karnavat, A., Marineau, J., McCallie, G. L., Olde, B. A., Terry, B. L., & Graesser, A.
(2001). Development of physics text corpora for latent semantic analysis. In Moore, J. & Stenning, K.
(Eds.), Proceedings of the 23rd Annual Conference of the Cognitive; Science Society, (pp. 297–300).,
Mahwah, NJ. Erlbaum.
Franceschetti, D., Karnavat, A., Marineau, J., McCallie, G. L., Olde, B. A., Terry, B. L., Graesser, A., &
C (2001). Development of physics text corpora for latent semantic analysis. In Moore, J. & Stenning,
K. (Eds.), Proceedings of the 23rd Annual Conference of the Cognitive Science Society, (pp. 297–300).,
Mahwah, NJ. Erlbaum.
Graesser, A., Chipman, P., Haynes, B., & Olney, A. (in press). AutoTutor: An intelligent tutoring system
with mixed-initiative dialogue. IEEE Transactions on Education.
Graesser, A. C., Lu, S., Jackson, G. T., Mitchell, H., Ventura, M., Olney, A., & Louwerse, M. M. (2004).
AutoTutor: A tutor with dialogue in natural language. Behavioral Research Methods, Instruments, and
Computers, 36, 180–193.
REFERENCES CITED–1

Graesser, A. C., VanLehn, K., Rose, C., Jordan, P., & Harter, D. (2001). Intelligent tutoring systems with
conversational dialogue. AI Magazine, 22, 39–51.
Hu, X., Cai, Z., Graesser, A., & Ventura, M. (2005). Similarity between semantic sSpaces. Preceding of
CogSci2005 - XXVII Annual Conference of the Cognitive Science Society,In B. Bara (Ed.), Proceedings of
the 27th Annual Meetings of the Cognitive Science Society. Mahwah, NJ: Erlbaum.
Hu, X., Cai, Z., Louwerse, M., Olney, A., Penumatsa, P., Graesser, A. C., & TRG (2003). An improved
LSA algorithm to evaluate student contributions in tutoring dialogue. In Gottlob, G. & Walsh, T. (Eds.),
Proceedings of the Eighteenth International Joint Conference on Arti cial Intelligence, (pp. 1489–1491).,
San Francisco. Morgan Kaufmann.
Kintsch, W. (1998). Comprehension: A Paradigm for Cognition. New York: Cambridge University Press.
Landauer, T. & Dumais, S. (1997). A solution to plato's problem: The latent semantic analysis theory of the
acquisition, induction, and representation of knowledge. Psychological Review, 104, 211–240.
Landauer, T., Laham, D., & Foltz, P. (1998). Computer-based grading of the conceptual content of essays.
Landauer, T. K., Foltz, P. W., & Laham, D. (1998). Introduction to latent semantic analysis. Discourse
Processes, 25, 259–284.
Lehn, K. V., Jones, R. M., & Chi, M. T. H. (1992). A model of the self-explanation effect. Journal of the
Learning Sciences, 2, 1–60.
Manning, C. & Schütze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge,
MA: MIT Press.
Olde, B. A.and Franceschetti, D., Karnavat; Graesser, A. C., & the Tutoring Research Group (2002). The
right stuff: Do you need to sanitize your corpus when using; latent semantic analysis? In Proceedings of
the 24th Annual Meeting of the Cognitive Science; Society Mahwah, NJ: Erlbaum, (pp. 708–713).
Olney, A. & Cai, Z. (2005a). An orthonormal basis for entailment. In Proceedings of the Eighteenth
International Florida Arti cial Intelligence Research Society Conference, (pp. 554–559)., Menlo Park,
CA. AAAI Press.
Olney, A. & Cai, Z. (2005b). An orthonormal basis for topic segmentation in tutorial dialogue. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural
Language Processing, (pp. 971–978)., Philadelphia. Association for Computational Linguistics.
Olney, A., Person, N., Louwerse, M., & Graesser, A. (2002). AutoTutor: A conversational tutoring environment. In Proceedings of the ACL-02 Demonstration Session, (pp. 108–109)., Philadelphia. Association
for Computational Linguistics.
Porter, M. (1980). An algorithm for suf x stripping. Program, 14(3) pp 130-137.
Salton, G. (1971). The Smart Retrieval System: Experiments in Automatic Document Processing. Englewood
Cliffs, NJ: Prentice-Hall.
Wolfe, M., Schreiner, M., Foltz, P., Kintsch, W., & Landauer, T. (1998). Learning from text: Matching
readers and texts by latent semantic analysis. Discourse Processes, 25(2&3), 309–336.

REFERENCES CITED–2

